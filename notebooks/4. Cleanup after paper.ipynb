{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Protocol\n",
    "\n",
    "import math, torch\n",
    "from typing import List, Tuple\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    #Meta\n",
    "    model_name: str\n",
    "    generation: str\n",
    "    token_type: str\n",
    "    #Training\n",
    "    epochs: int\n",
    "    dropout: float\n",
    "    #Model\n",
    "    layers: int\n",
    "    emb_size: int\n",
    "    heads: int\n",
    "    ff_dim: int\n",
    "    #Data \n",
    "    file_name: str\n",
    "    batch_size: int\n",
    "\n",
    "hp = HyperParams(\n",
    "    model_name=\"LocalTestForDelete\",\n",
    "    generation=\"V2\",\n",
    "    token_type=\"Word\",\n",
    "    file_name=\"spa\",\n",
    "    layers=3,\n",
    "    emb_size=256,\n",
    "    heads=8,\n",
    "    ff_dim=512,\n",
    "    epochs=10,\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "GH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = is_running_in_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if IS_COLAB else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_path(file_name: str):\n",
    "    if IS_COLAB:\n",
    "        url = f\"https://raw.githubusercontent.com/WillCable97/HomemadeTransformer/main/data/raw/{file_name}.txt\"\n",
    "        !wget {url}\n",
    "        return f\"./{file_name}.txt\"\n",
    "    else:\n",
    "        from homemadetransformer.config import DATA_DIR\n",
    "        return DATA_DIR / \"raw\" / f\"{file_name}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "class Tokenizer(Protocol):\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        ...\n",
    "\n",
    "    def add_special_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nm2z5GjcZ5L"
   },
   "source": [
    "## Simple Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"Simple word-based tokenizer that splits on whitespace.\"\"\"\n",
    "    \n",
    "    def __init__(self, sos_token=\"<SOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.lower().strip().split()\n",
    "    \n",
    "    def add_special_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.sos_token] + tokens + [self.eos_token]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPreprocessor:\n",
    "    def __init__(self, src_tok: Tokenizer | None = None, tgt_tok: Tokenizer | None = None):\n",
    "        self.src_tok = src_tok or SimpleWordTokenizer() #Default to this \n",
    "        self.tgt_tok = tgt_tok or SimpleWordTokenizer()\n",
    "        self.max_src_len = None\n",
    "        self.max_tgt_len = None\n",
    "\n",
    "        # will be filled by fit()\n",
    "        self.src2idx: dict[str, int] = {}\n",
    "        self.tgt2idx: dict[str, int] = {}\n",
    "        self.idx2src: dict[int, str] = {}\n",
    "        self.idx2tgt: dict[int, str] = {}\n",
    "        self.src_len = self.tgt_len = 0  # effective lengths\n",
    "\n",
    "    # ---------- helpers ------------------------------------------------------\n",
    "\n",
    "    def _tok_plus_specials(self, text: str, tkzr: Tokenizer) -> List[str]:\n",
    "        \"\"\"tokenise then add <SOS>/<EOS> (or whatever the tkzr decides).\"\"\"\n",
    "        return tkzr.add_special_tokens(tkzr.tokenize(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_vocab(sentences: List[List[str]], pad: str, unk: str) -> dict:\n",
    "        vocab = {pad: 0, unk: 1} #Reserve these from the beginning\n",
    "        for tok in chain.from_iterable(sentences):\n",
    "            vocab.setdefault(tok, len(vocab))\n",
    "        return vocab\n",
    "\n",
    "    def _encode_pad(self, tokens: List[str], vocab: dict\n",
    "                    , max_len: int, pad_idx: int) -> List[int]:\n",
    "        idxs = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens][:max_len]\n",
    "        idxs += [pad_idx] * (max_len - len(idxs))\n",
    "        return idxs\n",
    "\n",
    "    def src_ids(self, text: str) -> torch.LongTensor:\n",
    "        toks = self._tok_plus_specials(text, self.src_tok)\n",
    "        ids  = [self.src2idx.get(t, self.src2idx[self.src_tok.unk_token]) for t in toks]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def text_from_tgt(self, ids: torch.LongTensor) -> str:\n",
    "        return \" \".join(self.idx2tgt[i.item()] for i in ids)\n",
    "\n",
    "\n",
    "    # ---------- public ----------------------\n",
    "    def fit(self, pairs: List[Tuple[str, str]]) -> None: #Takes in pairs of sentences\n",
    "        #Do all the tokenisation\n",
    "        src_tokd = [self._tok_plus_specials(s, self.src_tok) for s, _ in pairs]\n",
    "        tgt_tokd = [self._tok_plus_specials(t, self.tgt_tok) for _, t in pairs]\n",
    "\n",
    "        self.src2idx = self._build_vocab(src_tokd, self.src_tok.pad_token, self.src_tok.unk_token)\n",
    "        self.tgt2idx = self._build_vocab(tgt_tokd, self.tgt_tok.pad_token, self.tgt_tok.unk_token)\n",
    "        self.idx2src = {i: w for w, i in self.src2idx.items()}\n",
    "        self.idx2tgt = {i: w for w, i in self.tgt2idx.items()}\n",
    "\n",
    "        # decide max effective lengths\n",
    "        self.src_len = self.max_src_len or max(map(len, src_tokd))\n",
    "        self.tgt_len = self.max_tgt_len or max(map(len, tgt_tokd))\n",
    "\n",
    "    def transform(self, pairs: List[Tuple[str, str]]):\n",
    "        pad_src = self.src2idx[self.src_tok.pad_token]\n",
    "        pad_tgt = self.tgt2idx[self.tgt_tok.pad_token]\n",
    "\n",
    "        src_enc = [\n",
    "            self._encode_pad(\n",
    "                self._tok_plus_specials(s, self.src_tok),\n",
    "                self.src2idx,\n",
    "                self.src_len,\n",
    "                pad_src,\n",
    "            )\n",
    "            for s, _ in pairs\n",
    "        ]\n",
    "        tgt_enc = [\n",
    "            self._encode_pad(\n",
    "                self._tok_plus_specials(t, self.tgt_tok),\n",
    "                self.tgt2idx,\n",
    "                self.tgt_len,\n",
    "                pad_tgt,\n",
    "            )\n",
    "            for _, t in pairs\n",
    "        ]\n",
    "        return src_enc, tgt_enc\n",
    "\n",
    "    def fit_transform(self, pairs: List[Tuple[str, str]]):\n",
    "        self.fit(pairs)\n",
    "        return self.transform(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for translation data.\"\"\"\n",
    "    \n",
    "    def __init__(self, src_data: List[List[int]], tgt_data: List[List[int]]):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx]), torch.tensor(self.tgt_data[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtUBZ07ccZ5R"
   },
   "source": [
    "## Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751767481322,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FXO2Hf7IcZ5S"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -math.log(10000.0) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1751767482233,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "r4qNq9bAcZ5S"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead,\n",
    "                 src_vocab_size, tgt_vocab_size, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, mem_pad_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        out = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                               None, src_pad_mask, tgt_pad_mask, mem_pad_mask)\n",
    "        return self.generator(out)    \n",
    "\n",
    "    def encode(self, src, src_pad_mask=None):\n",
    "        S = src.size(0)\n",
    "        src_mask = src.new_zeros((S, S), dtype=torch.bool)\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)),\n",
    "            src_mask,\n",
    "            src_key_padding_mask=src_pad_mask,      \n",
    "        )\n",
    "\n",
    "    def decode_step(self, tgt, memory, mem_pad_mask=None):\n",
    "        T = tgt.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(T, T, device=tgt.device, dtype=torch.bool), 1)\n",
    "        out = self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)),\n",
    "            memory,\n",
    "            tgt_mask,\n",
    "            memory_key_padding_mask=mem_pad_mask, \n",
    "        )\n",
    "        return self.generator(out)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing + Testing translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, sentence: str, pp, device, max_len: int = 50) -> str:\n",
    "    model.eval()\n",
    "    device   = torch.device(device)\n",
    "\n",
    "    sos, eos = pp.tgt2idx[pp.tgt_tok.sos_token], pp.tgt2idx[pp.tgt_tok.eos_token]\n",
    "\n",
    "    # encode source\n",
    "    src_ids = pp.src_ids(sentence).to(device)[:, None]    \n",
    "    src_pad_mask = (src_ids.squeeze(1) == pp.src2idx[pp.src_tok.pad_token])[None, :]\n",
    "    memory = model.encode(src_ids, src_pad_mask=src_pad_mask)# pass mask!!!!!                        \n",
    "\n",
    "    tgt_ids  = torch.tensor([[sos]], device=device)              \n",
    "\n",
    "    for _ in range(max_len):\n",
    "        #logits = model.decode_step(tgt_ids, memory) \n",
    "        logits = model.decode_step(tgt_ids, memory, mem_pad_mask=src_pad_mask)             \n",
    "        next_id = logits[-1].argmax(-1).item()# greedy\n",
    "\n",
    "        if next_id == eos:\n",
    "            break\n",
    "        tgt_ids = torch.cat([tgt_ids, torch.tensor([[next_id]], device=device)])\n",
    "\n",
    "    return pp.text_from_tgt(tgt_ids.squeeze(1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_translations(model, epoch: int, pp, device, max_len: int = 50):\n",
    "    # Sample sentences to translate\n",
    "    sample_sentences = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"I love this place.\",\n",
    "        \"What time is it?\",\n",
    "        \"Thank you very much.\",\n",
    "        \"Good morning everyone.\",\n",
    "        \"I'm excited about dinner tonight.\"\n",
    "    ]\n",
    "    \n",
    "    model.eval()\n",
    "    translations_text = f\"## Epoch {epoch} Sample Translations\\n\\n\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, src_sentence in enumerate(sample_sentences):\n",
    "            try:\n",
    "                # Translate using your existing translate function\n",
    "                translation = greedy_translate(model, src_sentence, pp, device, max_len)\n",
    "                \n",
    "                # Format for TensorBoard\n",
    "                translations_text += f\"**{src_sentence}** → **{translation}**\\n\\n\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                translations_text += f\"**{src_sentence}** → **Error: {str(e)}**\\n\\n\"\n",
    "    return translations_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogger:\n",
    "    def __init__(self, model_name):\n",
    "        self.writer  = torch.utils.tensorboard.SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "    def log_avg_loss(self, train_loss, val_loss, epoch):\n",
    "        self.writer.add_scalar('Loss/epoch_avg_train', train_loss, epoch)\n",
    "        self.writer.add_scalar('Loss/epoch_avg_val', val_loss, epoch)\n",
    "    \n",
    "    def log_step_level_loss(self, loss, step):\n",
    "        self.writer.add_scalar('Loss/train_step', loss, step)\n",
    "\n",
    "    def log_sample_translations(self, translations, epoch):\n",
    "        self.writer.add_text('Translations/Samples', translations, epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def log_lr(self, lr, epoch):\n",
    "        self.writer.add_scalar('Learning Rate/epoch_avg', lr, epoch)\n",
    "\n",
    "    def log_hyperparameters(self, hp, perplexity, bleu):\n",
    "        print(f\"Recieved hyperparameters : perplexity: {perplexity}, bleu: {bleu}\")\n",
    "        self.writer.add_hparams(hp, {\n",
    "            'perplexity': float(perplexity)\n",
    "            , 'bleu': float(bleu)}\n",
    "        )\n",
    "        self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device):\n",
    "    return torch.triu(torch.full((sz, sz), float('-inf'), device=device), 1)\n",
    "\n",
    "def create_mask(src, tgt, pad_idx=0):\n",
    "    src_seq_len = src.size(0)\n",
    "    tgt_seq_len = tgt.size(0)\n",
    "    device = src.device\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == pad_idx).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_forward_pass(model, src_batch, tgt_batch, pad_idx, device, loss_fn):\n",
    "  src_batch = src_batch.transpose(0, 1).to(device) \n",
    "  tgt_batch = tgt_batch.transpose(0, 1).to(device) \n",
    "\n",
    "  tgt_input = tgt_batch[:-1, :]\n",
    "  tgt_output = tgt_batch[1:, :]\n",
    "\n",
    "  src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src_batch, tgt_input, pad_idx)\n",
    "\n",
    "  logits = model(src_batch, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
    "\n",
    "  logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "  tgt_output_flat = tgt_output.reshape(-1)\n",
    "\n",
    "  loss = loss_fn(logits_flat, tgt_output_flat)\n",
    "\n",
    "  return loss, logits_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_github(local_path, repo_path, token, username=\"WillCable97\", repo=\"HomemadeTransformer\", branch=\"main\"):\n",
    "    \n",
    "    with open(local_path, \"rb\") as f:\n",
    "        content = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{username}/{repo}/contents/{repo_path}\"\n",
    "    data = {\n",
    "        \"message\": f\"Upload {repo_path} from Colab\",\n",
    "        \"content\": content,\n",
    "        \"branch\": branch\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    r = requests.put(url, headers=headers, json=data)\n",
    "    if r.status_code == 201:\n",
    "        print(f\"✅ Uploaded {repo_path} to GitHub\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to upload {repo_path} — {r.status_code}: {r.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduel (from paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_lr_scheduler(optimizer, d_model=512, warmup_steps=WARMUP_STEPS):\n",
    "    def lr_lambda(step: int):\n",
    "        step = max(step, 1)\n",
    "        scale = (d_model ** -0.5)           # keep the usual d_model-0.5 term\n",
    "        return scale * min(step ** -0.5,\n",
    "                           step * (warmup_steps ** -1.5))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Helper func\n",
    "def ids_to_tokens(ids: List[int], idx2tok: dict[int, str]\n",
    "                  , eos_token: str, pad_token: str,) -> List[str]:\n",
    "    out = []\n",
    "    for i in ids:\n",
    "        tok = idx2tok[i]\n",
    "        if tok == eos_token or tok == pad_token:\n",
    "            break\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model: torch.nn.Module, dataloader\n",
    "                       , pp, loss_fn, device: str = \"cpu\",) -> float:\n",
    "    \"\"\"\n",
    "    Returns corpus-level perplexity = exp(mean cross-entropy).\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model.eval()\n",
    "\n",
    "    pad_idx = pp.tgt2idx[pp.tgt_tok.pad_token]\n",
    "    total_log_loss, total_tokens = 0.0, 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        # count **non-PAD** target tokens (excluding first token)\n",
    "        tgt_out = tgt[1:]\n",
    "        n_tokens = (tgt_out != pad_idx).sum().item()\n",
    "\n",
    "        loss, _ = create_batch_forward_pass(\n",
    "            model, src, tgt,\n",
    "            pad_idx=pad_idx,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_log_loss += loss.item() * n_tokens\n",
    "        total_tokens   += n_tokens\n",
    "\n",
    "    mean_nll = total_log_loss / total_tokens\n",
    "    return math.exp(mean_nll)                  # perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_bleu(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader,\n",
    "    pp,                             \n",
    "    max_len: int = 50,\n",
    "    device: str = \"cpu\",\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Returns corpus BLEU-4 (same default as SacreBLEU) using greedy decoding.\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model.eval()\n",
    "    smoothie = SmoothingFunction().method4      \n",
    "\n",
    "    pad_tok = pp.tgt_tok.pad_token\n",
    "    eos_tok = pp.tgt_tok.eos_token\n",
    "\n",
    "    refs: List[List[List[str]]] = []\n",
    "    hyps: List[List[str]]      = []\n",
    "\n",
    "    for src_batch, tgt_batch in dataloader:\n",
    "        for b in range(src_batch.size(0)):\n",
    "            src_ids = src_batch[b].tolist()\n",
    "            tgt_ids = tgt_batch[b].tolist()\n",
    "\n",
    "            src_tokens = ids_to_tokens(\n",
    "                src_ids,\n",
    "                idx2tok=pp.idx2src,\n",
    "                eos_token=pp.src_tok.eos_token,\n",
    "                pad_token=pp.src_tok.pad_token,\n",
    "            )\n",
    "            src_sentence = \" \".join(src_tokens)\n",
    "\n",
    "            hyp_sentence = greedy_translate(\n",
    "                model, src_sentence, pp,\n",
    "                max_len=max_len, device=device,\n",
    "            )\n",
    "            hyps.append(hyp_sentence.split())\n",
    "\n",
    "            ref_tokens = ids_to_tokens(\n",
    "                tgt_ids,\n",
    "                idx2tok=pp.idx2tgt,\n",
    "                eos_token=eos_tok,\n",
    "                pad_token=pad_tok,\n",
    "            )\n",
    "            refs.append([ref_tokens])\n",
    "\n",
    "    return corpus_bleu(refs, hyps, smoothing_function=smoothie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create All Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "text_path = get_text_path(hp.file_name)\n",
    "\n",
    "with open(text_path, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")[:1000]\n",
    "\n",
    "pairs = [tuple(line.split(\"\\t\")[:2]) for line in lines if \"\\t\" in line] \n",
    "\n",
    "\n",
    "#Pre process + Prepare\n",
    "preprocessor = TranslationPreprocessor(\n",
    "    src_tok=SimpleWordTokenizer(),\n",
    "    tgt_tok=SimpleWordTokenizer()\n",
    ")\n",
    "\n",
    "src_data, tgt_data = preprocessor.fit_transform(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Create data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets with fixed random seed\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(src_data))\n",
    "train_size = int(0.9 * len(src_data))\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_src_data = [src_data[i] for i in train_indices]\n",
    "train_tgt_data = [tgt_data[i] for i in train_indices]\n",
    "val_src_data = [src_data[i] for i in val_indices]\n",
    "val_tgt_data = [tgt_data[i] for i in val_indices]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TranslationDataset(train_src_data, train_tgt_data)\n",
    "val_dataset = TranslationDataset(val_src_data, val_tgt_data)\n",
    "train_dl = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=hp.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Training and model Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=hp.layers,\n",
    "    num_decoder_layers=hp.layers,\n",
    "    emb_size=hp.emb_size,\n",
    "    nhead=hp.heads,\n",
    "    src_vocab_size=len(preprocessor.src2idx),\n",
    "    tgt_vocab_size=len(preprocessor.tgt2idx),\n",
    "    dim_feedforward=hp.ff_dim,\n",
    "    dropout=hp.dropout \n",
    ")\n",
    "\n",
    "MODEL_NAME_TIME = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "my_logger = MyLogger(MODEL_NAME_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4- Learning and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1, betas=(0.9, 0.98),eps=1e-9)\n",
    "#lr_scheduler = transformer_lr_scheduler(optimizer, d_model=hp.emb_size, warmup_steps=4000)\n",
    "\n",
    "def avg_tokens_per_step(loader, pad_id, n_batches=100):\n",
    "    \"\"\"Quick probe – look at the first n_batches of your DataLoader.\"\"\"\n",
    "    import itertools, torch\n",
    "    tot = 0\n",
    "    for i, (src, tgt) in enumerate(loader):\n",
    "        if i == n_batches: break\n",
    "        tot += (src != pad_id).sum().item() + (tgt != pad_id).sum().item()\n",
    "    return tot // n_batches        # integer average\n",
    "\n",
    "T_PAPER = 50_000\n",
    "T_NEW   = avg_tokens_per_step(train_dl, pad_id=0)   # <- run once\n",
    "\n",
    "# Scaling the LR based on my thing\n",
    "LR_MULT      = T_NEW / T_PAPER\n",
    "WARMUP_STEPS = int(4_000 * T_PAPER / T_NEW)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=LR_MULT,                     # paper used 1.0 → scale it\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n",
    "\n",
    "lr_scheduler = transformer_lr_scheduler(\n",
    "    optimizer,\n",
    "    d_model=hp.emb_size,\n",
    "    warmup_steps=WARMUP_STEPS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, val_dataloader, preprocessor, optimizer, lr_scheduler, loss_fn, num_epochs, my_logger, pad_idx=0, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    global_step = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # For Logging progression\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        dl_len = len(dataloader)\n",
    "        i = 1\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for eng_batch, spa_batch in dataloader:\n",
    "            \n",
    "            print(f\"\\rStarting {i}/{dl_len}\", end=\"\")\n",
    "            i += 1\n",
    "            global_step += 1\n",
    "\n",
    "            loss, _ = create_batch_forward_pass(model, eng_batch, spa_batch, pad_idx, device, loss_fn)\n",
    "\n",
    "            #Perform the training step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            my_logger.log_step_level_loss(loss.item(), global_step)\n",
    "\n",
    "            # Log learning rate\n",
    "            current_lr = lr_scheduler.get_last_lr()[0]\n",
    "            my_logger.log_lr(current_lr, global_step)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eng_batch, spa_batch in val_dataloader:\n",
    "                loss, _ = create_batch_forward_pass(model, eng_batch, spa_batch, pad_idx, device, loss_fn)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        my_logger.log_avg_loss(avg_loss, avg_val_loss, epoch)\n",
    "\n",
    "        #Log sample translations\n",
    "        sample_translations = create_sample_translations(model, epoch, preprocessor, device)\n",
    "        my_logger.log_sample_translations(sample_translations, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} — Train Loss: {avg_loss:.4f} — Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    #Log hyperparameters\n",
    "    print(\"Logging hyperparameters and evaluation metrics\")\n",
    "    perplexity = compute_perplexity(model, val_dataloader, preprocessor, loss_fn, device)\n",
    "    bleu = compute_bleu(model, val_dataloader, preprocessor, device=device)\n",
    "    print(f\"Calculated perplexity: {perplexity} and bleu: {bleu}\")\n",
    "    my_logger.log_hyperparameters(asdict(hp), perplexity, bleu)\n",
    "\n",
    "    my_logger.close()\n",
    "    \n",
    "    return avg_loss, avg_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530310,
     "status": "ok",
     "timestamp": 1751768025121,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FCwVbyWBcZ5T",
    "outputId": "1bac3405-2569-4c00-b4e2-d314ad31e519"
   },
   "outputs": [],
   "source": [
    "# Train it\n",
    "losses = train_model(model, train_dl, val_dl, preprocessor\n",
    "                     , optimizer, lr_scheduler\n",
    "                     , nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "                     , num_epochs=hp.epochs, my_logger=my_logger, pad_idx=0\n",
    "                     , device=DEVICE)\n",
    "\n",
    "\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "ROOT_PATH = \"/content/drive/MyDrive/Colab Projects/Translation transformer/\"\n",
    "torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Projects/Translation transformer/{MODEL_NAME_TIME}.pth')\n",
    "#torch.save(model.state_dict(), f'./models/{MODEL_NAME_TIME}.pth')\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def upload_run_dir(run_dir: Path, dest_root: str, gh_token: str):\n",
    "    \"\"\"Uploads *all* TensorBoard event files in `run_dir`, incl. hparams sub-dir.\"\"\"\n",
    "    for ev in sorted(run_dir.rglob(\"events.out.tfevents.*\")):\n",
    "        rel_dest = Path(dest_root) / ev.relative_to(run_dir)   # keep same tree\n",
    "        upload_to_github(str(ev), str(rel_dest), gh_token)\n",
    "\n",
    "# --- in your Colab block ---\n",
    "if IS_COLAB:\n",
    "    log_dir = Path(f\"./runs/{MODEL_NAME_TIME}\")\n",
    "    upload_run_dir(\n",
    "        log_dir,\n",
    "        f\"./notebooks/runs/{MODEL_NAME_TIME}\",\n",
    "        GH_TOKEN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
