{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParams:\n",
    "    #Meta\n",
    "    model_name: str\n",
    "    token_type: str\n",
    "    #Training\n",
    "    epochs: int\n",
    "    dropout: float\n",
    "    #Model\n",
    "    layers: int\n",
    "    emb_size: int\n",
    "    heads: int\n",
    "    ff_dim: int\n",
    "    #Data \n",
    "    file_name: str\n",
    "    batch_size: int\n",
    "\n",
    "hp = HyperParams(\n",
    "    model_name=\"LocalTestForDelete\",\n",
    "    token_type=\"Word\",\n",
    "    file_name=\"spa\",\n",
    "    layers=3,\n",
    "    emb_size=256,\n",
    "    heads=8,\n",
    "    ff_dim=512,\n",
    "    epochs=10,\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "GH_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = is_running_in_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if IS_COLAB else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_path(file_name: str):\n",
    "    if IS_COLAB:\n",
    "        url = f\"https://raw.githubusercontent.com/WillCable97/HomemadeTransformer/main/data/raw/{file_name}.txt\"\n",
    "        !wget {url}\n",
    "        return f\"./{file_name}.txt\"\n",
    "    else:\n",
    "        from homemadetransformer.config import DATA_DIR\n",
    "        return DATA_DIR / \"raw\" / f\"{file_name}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Protocol\n",
    "\n",
    "class Tokenizer(Protocol):\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        ...\n",
    "\n",
    "    def add_special_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nm2z5GjcZ5L"
   },
   "source": [
    "## Simple Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"Simple word-based tokenizer that splits on whitespace.\"\"\"\n",
    "    \n",
    "    def __init__(self, sos_token=\"<SOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        return text.lower().strip().split()\n",
    "    \n",
    "    def add_special_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        return [self.sos_token] + tokens + [self.eos_token]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPreprocessor:\n",
    "    def __init__(self, src_tok: Tokenizer | None = None, tgt_tok: Tokenizer | None = None):\n",
    "        self.src_tok = src_tok or SimpleWordTokenizer() #Default to this \n",
    "        self.tgt_tok = tgt_tok or SimpleWordTokenizer()\n",
    "        self.max_src_len = None\n",
    "        self.max_tgt_len = None\n",
    "\n",
    "        # will be filled by fit()\n",
    "        self.src2idx: dict[str, int] = {}\n",
    "        self.tgt2idx: dict[str, int] = {}\n",
    "        self.idx2src: dict[int, str] = {}\n",
    "        self.idx2tgt: dict[int, str] = {}\n",
    "        self.src_len = self.tgt_len = 0  # effective lengths\n",
    "\n",
    "    # ---------- helpers ------------------------------------------------------\n",
    "\n",
    "    def _tok_plus_specials(self, text: str, tkzr: Tokenizer) -> List[str]:\n",
    "        \"\"\"tokenise then add <SOS>/<EOS> (or whatever the tkzr decides).\"\"\"\n",
    "        return tkzr.add_special_tokens(tkzr.tokenize(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def _build_vocab(sentences: List[List[str]], pad: str, unk: str) -> dict:\n",
    "        vocab = {pad: 0, unk: 1} #Reserve these from the beginning\n",
    "        for tok in chain.from_iterable(sentences):\n",
    "            vocab.setdefault(tok, len(vocab))\n",
    "        return vocab\n",
    "\n",
    "    def _encode_pad(self, tokens: List[str], vocab: dict\n",
    "                    , max_len: int, pad_idx: int) -> List[int]:\n",
    "        idxs = [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens][:max_len]\n",
    "        idxs += [pad_idx] * (max_len - len(idxs))\n",
    "        return idxs\n",
    "\n",
    "    def src_ids(self, text: str) -> torch.LongTensor:\n",
    "        toks = self._tok_plus_specials(text, self.src_tok)\n",
    "        ids  = [self.src2idx.get(t, self.src2idx[self.src_tok.unk_token]) for t in toks]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "    def text_from_tgt(self, ids: torch.LongTensor) -> str:\n",
    "        return \" \".join(self.idx2tgt[i.item()] for i in ids)\n",
    "\n",
    "\n",
    "    # ---------- public ----------------------\n",
    "    def fit(self, pairs: List[Tuple[str, str]]) -> None: #Takes in pairs of sentences\n",
    "        #Do all the tokenisation\n",
    "        src_tokd = [self._tok_plus_specials(s, self.src_tok) for s, _ in pairs]\n",
    "        tgt_tokd = [self._tok_plus_specials(t, self.tgt_tok) for _, t in pairs]\n",
    "\n",
    "        self.src2idx = self._build_vocab(src_tokd, self.src_tok.pad_token, self.src_tok.unk_token)\n",
    "        self.tgt2idx = self._build_vocab(tgt_tokd, self.tgt_tok.pad_token, self.tgt_tok.unk_token)\n",
    "        self.idx2src = {i: w for w, i in self.src2idx.items()}\n",
    "        self.idx2tgt = {i: w for w, i in self.tgt2idx.items()}\n",
    "\n",
    "        # decide max effective lengths\n",
    "        self.src_len = self.max_src_len or max(map(len, src_tokd))\n",
    "        self.tgt_len = self.max_tgt_len or max(map(len, tgt_tokd))\n",
    "\n",
    "    def transform(self, pairs: List[Tuple[str, str]]):\n",
    "        pad_src = self.src2idx[self.src_tok.pad_token]\n",
    "        pad_tgt = self.tgt2idx[self.tgt_tok.pad_token]\n",
    "\n",
    "        src_enc = [\n",
    "            self._encode_pad(\n",
    "                self._tok_plus_specials(s, self.src_tok),\n",
    "                self.src2idx,\n",
    "                self.src_len,\n",
    "                pad_src,\n",
    "            )\n",
    "            for s, _ in pairs\n",
    "        ]\n",
    "        tgt_enc = [\n",
    "            self._encode_pad(\n",
    "                self._tok_plus_specials(t, self.tgt_tok),\n",
    "                self.tgt2idx,\n",
    "                self.tgt_len,\n",
    "                pad_tgt,\n",
    "            )\n",
    "            for _, t in pairs\n",
    "        ]\n",
    "        return src_enc, tgt_enc\n",
    "\n",
    "    def fit_transform(self, pairs: List[Tuple[str, str]]):\n",
    "        self.fit(pairs)\n",
    "        return self.transform(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for translation data.\"\"\"\n",
    "    \n",
    "    def __init__(self, src_data: List[List[int]], tgt_data: List[List[int]]):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx]), torch.tensor(self.tgt_data[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtUBZ07ccZ5R"
   },
   "source": [
    "## Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751767481322,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FXO2Hf7IcZ5S"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -math.log(10000.0) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1751767482233,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "r4qNq9bAcZ5S"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead,\n",
    "                 src_vocab_size, tgt_vocab_size, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, mem_pad_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        out = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                               None, src_pad_mask, tgt_pad_mask, mem_pad_mask)\n",
    "        return self.generator(out)\n",
    "    \n",
    "    def encode(self, src):                    \n",
    "        src_mask = src.new_zeros((src.size(0), src.size(0)), dtype=torch.bool)\n",
    "        return self.transformer.encoder(\n",
    "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
    "        )\n",
    "\n",
    "    def decode_step(self, tgt, memory):      \n",
    "        T = tgt.size(0)\n",
    "        tgt_mask = torch.triu(torch.ones(T, T, device=tgt.device, dtype=torch.bool), 1)\n",
    "        out = self.transformer.decoder(\n",
    "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
    "        )\n",
    "        return self.generator(out)           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing + Testing translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_translate(model, sentence: str, pp, max_len: int = 50, device=\"cpu\") -> str:\n",
    "    model.eval()\n",
    "    device   = torch.device(device)\n",
    "\n",
    "    sos, eos = pp.tgt2idx[pp.tgt_tok.sos_token], pp.tgt2idx[pp.tgt_tok.eos_token]\n",
    "\n",
    "    # encode source\n",
    "    src_ids  = pp.src_ids(sentence).to(device)[:, None]          \n",
    "    memory   = model.encode(src_ids)                             \n",
    "\n",
    "    tgt_ids  = torch.tensor([[sos]], device=device)              \n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model.decode_step(tgt_ids, memory)              \n",
    "        next_id = logits[-1].argmax(-1).item()# greedy\n",
    "\n",
    "        if next_id == eos:\n",
    "            break\n",
    "        tgt_ids = torch.cat([tgt_ids, torch.tensor([[next_id]], device=device)])\n",
    "\n",
    "    return pp.text_from_tgt(tgt_ids.squeeze(1)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_translations(model, epoch: int, pp, device, max_len: int = 50):\n",
    "    # Sample sentences to translate\n",
    "    sample_sentences = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"I love this place.\",\n",
    "        \"What time is it?\",\n",
    "        \"Thank you very much.\",\n",
    "        \"Good morning everyone.\",\n",
    "        \"I'm excited about dinner tonight.\"\n",
    "    ]\n",
    "    \n",
    "    model.eval()\n",
    "    translations_text = f\"## Epoch {epoch} Sample Translations\\n\\n\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, src_sentence in enumerate(sample_sentences):\n",
    "            try:\n",
    "                # Translate using your existing translate function\n",
    "                translation = greedy_translate(model, src_sentence, pp, max_len, device)\n",
    "                \n",
    "                # Format for TensorBoard\n",
    "                translations_text += f\"**{src_sentence}** → **{translation}**\\n\\n\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                translations_text += f\"**{src_sentence}** → **Error: {str(e)}**\\n\\n\"\n",
    "    return translations_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogger:\n",
    "    def __init__(self, model_name):\n",
    "        self.writer  = torch.utils.tensorboard.SummaryWriter(f'runs/{model_name}')\n",
    "\n",
    "    def log_avg_loss(self, train_loss, val_loss, epoch):\n",
    "        self.writer.add_scalar('Loss/epoch_avg_train', train_loss, epoch)\n",
    "        self.writer.add_scalar('Loss/epoch_avg_val', val_loss, epoch)\n",
    "    \n",
    "    def log_step_level_loss(self, loss, step):\n",
    "        self.writer.add_scalar('Loss/train_step', loss, step)\n",
    "\n",
    "    def log_sample_translations(self, translations, epoch):\n",
    "        self.writer.add_text('Translations/Samples', translations, epoch)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_forward_pass(model, src_batch, tgt_batch, pad_idx, device, loss_fn):\n",
    "  src_batch = src_batch.transpose(0, 1).to(device) \n",
    "  tgt_batch = tgt_batch.transpose(0, 1).to(device) \n",
    "\n",
    "  tgt_input = tgt_batch[:-1, :]\n",
    "  tgt_output = tgt_batch[1:, :]\n",
    "\n",
    "  src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(src_batch, tgt_input, pad_idx)\n",
    "\n",
    "  logits = model(src_batch, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
    "\n",
    "  logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "  tgt_output_flat = tgt_output.reshape(-1)\n",
    "\n",
    "  loss = loss_fn(logits_flat, tgt_output_flat)\n",
    "\n",
    "  return loss, logits_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_github(local_path, repo_path, token, username=\"WillCable97\", repo=\"HomemadeTransformer\", branch=\"main\"):\n",
    "    \n",
    "    with open(local_path, \"rb\") as f:\n",
    "        content = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{username}/{repo}/contents/{repo_path}\"\n",
    "    data = {\n",
    "        \"message\": f\"Upload {repo_path} from Colab\",\n",
    "        \"content\": content,\n",
    "        \"branch\": branch\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    r = requests.put(url, headers=headers, json=data)\n",
    "    if r.status_code == 201:\n",
    "        print(f\"✅ Uploaded {repo_path} to GitHub\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to upload {repo_path} — {r.status_code}: {r.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_33YYkdUcZ5T"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751767490321,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "MHSvj9KwcZ5T"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, val_dataloader, preprocessor, optimizer, loss_fn, num_epochs, my_logger, pad_idx=0, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    global_step = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # For Logging progression\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        dl_len = len(dataloader)\n",
    "        i = 1\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for eng_batch, spa_batch in dataloader:\n",
    "            \n",
    "            print(f\"\\rStarting {i}/{dl_len}\", end=\"\")\n",
    "            i += 1\n",
    "            global_step += 1\n",
    "\n",
    "            loss, _ = create_batch_forward_pass(model, eng_batch, spa_batch, pad_idx, device, loss_fn)\n",
    "\n",
    "            #Perform the training step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            my_logger.log_step_level_loss(loss.item(), global_step)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eng_batch, spa_batch in val_dataloader:\n",
    "                loss, _ = create_batch_forward_pass(model, eng_batch, spa_batch, pad_idx, device, loss_fn)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        my_logger.log_avg_loss(avg_loss, avg_val_loss, epoch)\n",
    "\n",
    "        #Log sample translations\n",
    "        sample_translations = create_sample_translations(model, epoch, preprocessor, device)\n",
    "        my_logger.log_sample_translations(sample_translations, epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} — Train Loss: {avg_loss:.4f} — Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    my_logger.close()\n",
    "    \n",
    "    return avg_loss, avg_val_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create All Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "text_path = get_text_path(hp.file_name)\n",
    "\n",
    "with open(text_path, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")[:1000]\n",
    "\n",
    "pairs = [tuple(line.split(\"\\t\")[:2]) for line in lines if \"\\t\" in line] \n",
    "\n",
    "\n",
    "#Pre process + Prepare\n",
    "preprocessor = TranslationPreprocessor(\n",
    "    src_tok=SimpleWordTokenizer(),\n",
    "    tgt_tok=SimpleWordTokenizer()\n",
    ")\n",
    "\n",
    "src_data, tgt_data = preprocessor.fit_transform(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Create data Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and validation sets with fixed random seed\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(src_data))\n",
    "train_size = int(0.9 * len(src_data))\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_src_data = [src_data[i] for i in train_indices]\n",
    "train_tgt_data = [tgt_data[i] for i in train_indices]\n",
    "val_src_data = [src_data[i] for i in val_indices]\n",
    "val_tgt_data = [tgt_data[i] for i in val_indices]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TranslationDataset(train_src_data, train_tgt_data)\n",
    "val_dataset = TranslationDataset(val_src_data, val_tgt_data)\n",
    "train_dl = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=hp.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Training and mdel Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=hp.layers,\n",
    "    num_decoder_layers=hp.layers,\n",
    "    emb_size=hp.emb_size,\n",
    "    nhead=hp.heads,\n",
    "    src_vocab_size=len(preprocessor.src_vocab),\n",
    "    tgt_vocab_size=len(preprocessor.tgt_vocab),\n",
    "    dim_feedforward=hp.ff_dim,\n",
    "    dropout=hp.dropout \n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530310,
     "status": "ok",
     "timestamp": 1751768025121,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FCwVbyWBcZ5T",
    "outputId": "1bac3405-2569-4c00-b4e2-d314ad31e519"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting 1/29"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 29/29Logged 5 sample translations for epoch 0\n",
      "Epoch 1/10 — Train Loss: 5.1017 — Val Loss: 4.5857\n",
      "Starting epoch 1\n",
      "Starting 29/29Logged 5 sample translations for epoch 1\n",
      "Epoch 2/10 — Train Loss: 4.5807 — Val Loss: 4.5208\n",
      "Starting epoch 2\n",
      "Starting 29/29Logged 5 sample translations for epoch 2\n",
      "Epoch 3/10 — Train Loss: 4.3703 — Val Loss: 4.4191\n",
      "Starting epoch 3\n",
      "Starting 29/29Logged 5 sample translations for epoch 3\n",
      "Epoch 4/10 — Train Loss: 4.1559 — Val Loss: 4.1696\n",
      "Starting epoch 4\n",
      "Starting 2/29"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m loss_fn = nn.CrossEntropyLoss(ignore_index=\u001b[32m0\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_COLAB:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m#Save Log file\u001b[39;00m\n\u001b[32m     22\u001b[39m     log_dir= Path(\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, val_dataloader, preprocessor, optimizer, loss_fn, num_epochs, pad_idx, device)\u001b[39m\n\u001b[32m     50\u001b[39m tgt_output_flat = tgt_output.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m loss = loss_fn(logits_flat, tgt_output_flat)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m optimizer.step()\n\u001b[32m     56\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train it\n",
    "losses = train_model(model, train_dl, val_dl, preprocessor\n",
    "                     , optimizer, loss_fn\n",
    "                     , num_epochs=EPOCHS, pad_idx=0\n",
    "                     , device=DEVICE)\n",
    "\n",
    "\n",
    "#torch.save(model.state_dict(), f'./models/{model_name}.pth')\n",
    "\n",
    "\n",
    "if IS_COLAB:\n",
    "    #Save Log file\n",
    "    log_dir= Path(F\"./runs/{MODEL_NAME}\")\n",
    "    latest_event_file = max(log_dir.glob(\"events.out.tfevents.*\"), key=lambda f: f.stat().st_mtime, default=None)\n",
    "    upload_to_github(latest_event_file,\n",
    "                      f\"./notebooks/runs/{MODEL_NAME}/{latest_event_file.name}\"\n",
    "                      , GH_TOKEN)\n",
    "    \n",
    "    #Do the model\n",
    "    upload_to_github(f\"./models/{MODEL_NAME}.pth\",\n",
    "                      f\"./notebooks/models/{MODEL_NAME}.pth\"\n",
    "                      , GH_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
