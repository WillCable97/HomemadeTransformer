{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GH_TOKEN = \"\"\n",
    "\n",
    "MODEL_NAME = \"LocalTestForDelete\"\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "FILE_NAME = \"spa\"\n",
    "BATCH_SIZE = 32\n",
    "LAYERS = 3\n",
    "EMB_SIZE = 256\n",
    "HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_running_in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "IS_COLAB = is_running_in_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if IS_COLAB else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_path(file_name: str):\n",
    "    if IS_COLAB:\n",
    "        url = f\"https://raw.githubusercontent.com/WillCable97/HomemadeTransformer/main/data/raw/{file_name}.txt\"\n",
    "        !wget {url}\n",
    "        return f\"./{file_name}.txt\"\n",
    "    else:\n",
    "        from homemadetransformer.config import DATA_DIR\n",
    "        return DATA_DIR / \"raw\" / f\"{file_name}.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Nm2z5GjcZ5L"
   },
   "source": [
    "## Simple Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWordTokenizer:\n",
    "    \"\"\"Simple word-based tokenizer that splits on whitespace.\"\"\"\n",
    "    \n",
    "    def __init__(self, sos_token=\"<SOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\", unk_token=\"<UNK>\"):\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text by splitting on whitespace.\"\"\"\n",
    "        return text.lower().strip().split()\n",
    "    \n",
    "    def add_special_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Add SOS and EOS tokens to a sequence.\"\"\"\n",
    "        return [self.sos_token] + tokens + [self.eos_token]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPreprocessor:\n",
    "    \"\"\"Preprocessor for translation datasets with configurable tokenization.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 src_tokenizer=None, \n",
    "                 tgt_tokenizer=None,\n",
    "                 max_src_len: Optional[int] = None,\n",
    "                 max_tgt_len: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            src_tokenizer: Tokenizer for source language (defaults to SimpleWordTokenizer)\n",
    "            tgt_tokenizer: Tokenizer for target language (defaults to SimpleWordTokenizer)\n",
    "            src_lang: Source language code\n",
    "            tgt_lang: Target language code\n",
    "            max_src_len: Maximum source sequence length (None for auto)\n",
    "            max_tgt_len: Maximum target sequence length (None for auto)\n",
    "        \"\"\"\n",
    "        self.src_tokenizer = src_tokenizer or SimpleWordTokenizer()\n",
    "        self.tgt_tokenizer = tgt_tokenizer or SimpleWordTokenizer()\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        \n",
    "        # Vocabulary mappings\n",
    "        self.src_vocab = {}\n",
    "        self.tgt_vocab = {}\n",
    "        self.src_vocab_to_idx = {}\n",
    "        self.tgt_vocab_to_idx = {}\n",
    "        self.src_idx_to_vocab = {}\n",
    "        self.tgt_idx_to_vocab = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.src_vocab_size = 0\n",
    "        self.tgt_vocab_size = 0\n",
    "        self.max_src_len_actual = 0\n",
    "        self.max_tgt_len_actual = 0\n",
    "        \n",
    "    def clean_translation_pair(self, line: str) -> Optional[Tuple[str, str]]:\n",
    "        \"\"\"Clean a translation pair from the dataset.\"\"\"\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        \n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "            \n",
    "        src_text = parts[0].strip()\n",
    "        tgt_text = parts[1].strip().split(\"CC-BY\")[0].strip()  # Remove attribution text\n",
    "        \n",
    "        return src_text, tgt_text\n",
    "    \n",
    "    def build_vocabularies(self, src_sentences: List[str], tgt_sentences: List[str]):\n",
    "        \"\"\"Build vocabulary mappings from tokenized sentences.\"\"\"\n",
    "        # Tokenize all sentences\n",
    "        src_tokenized = [self.src_tokenizer.add_special_tokens(self.src_tokenizer.tokenize(sent)) \n",
    "                        for sent in src_sentences]\n",
    "        tgt_tokenized = [self.tgt_tokenizer.add_special_tokens(self.tgt_tokenizer.tokenize(sent)) \n",
    "                        for sent in tgt_sentences]\n",
    "        \n",
    "        # Build source vocabulary\n",
    "        src_vocab = set(chain.from_iterable(src_tokenized))\n",
    "        self.src_vocab_to_idx = {word: i for i, word in enumerate(src_vocab)}\n",
    "        self.src_idx_to_vocab = {i: word for word, i in self.src_vocab_to_idx.items()}\n",
    "        self.src_vocab_size = len(src_vocab)\n",
    "        \n",
    "        # Build target vocabulary\n",
    "        tgt_vocab = set(chain.from_iterable(tgt_tokenized))\n",
    "        self.tgt_vocab_to_idx = {word: i for i, word in enumerate(tgt_vocab)}\n",
    "        self.tgt_idx_to_vocab = {i: word for word, i in self.tgt_vocab_to_idx.items()}\n",
    "        self.tgt_vocab_size = len(tgt_vocab)\n",
    "        \n",
    "        # Calculate max lengths\n",
    "        self.max_src_len_actual = max(len(s) for s in src_tokenized)\n",
    "        self.max_tgt_len_actual = max(len(s) for s in tgt_tokenized)\n",
    "        \n",
    "        # Use provided max lengths if specified\n",
    "        if self.max_src_len is not None:\n",
    "            self.max_src_len_actual = min(self.max_src_len_actual, self.max_src_len)\n",
    "        if self.max_tgt_len is not None:\n",
    "            self.max_tgt_len_actual = min(self.max_tgt_len_actual, self.max_tgt_len)\n",
    "            \n",
    "        return src_tokenized, tgt_tokenized\n",
    "    \n",
    "    def encode_sentence(self, sentence: List[str], vocab_to_idx: Dict[str, int]) -> List[int]:\n",
    "        \"\"\"Encode a tokenized sentence to indices.\"\"\"\n",
    "        return [vocab_to_idx.get(word, vocab_to_idx.get(self.src_tokenizer.unk_token, 0)) \n",
    "                for word in sentence]\n",
    "    \n",
    "    def pad_sequence(self, seq: List[int], max_len: int, pad_idx: int = 0) -> List[int]:\n",
    "        \"\"\"Pad a sequence to the specified length.\"\"\"\n",
    "        return seq + [pad_idx] * (max_len - len(seq))\n",
    "    \n",
    "    def preprocess_data(self, raw_data: List[str]) -> Tuple[List[List[int]], List[List[int]]]:\n",
    "        \"\"\"\n",
    "        Preprocess raw translation data.\n",
    "        \n",
    "        Args:\n",
    "            raw_data: List of tab-separated translation pairs\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (encoded_src_sentences, encoded_tgt_sentences)\n",
    "        \"\"\"\n",
    "        # Clean the data\n",
    "        clean_pairs = [self.clean_translation_pair(line) for line in raw_data]\n",
    "        clean_pairs = [pair for pair in clean_pairs if pair is not None]\n",
    "        \n",
    "        # Split into source and target\n",
    "        src_sentences = [pair[0] for pair in clean_pairs]\n",
    "        tgt_sentences = [pair[1] for pair in clean_pairs]\n",
    "        \n",
    "        # Build vocabularies and tokenize\n",
    "        src_tokenized, tgt_tokenized = self.build_vocabularies(src_sentences, tgt_sentences)\n",
    "        \n",
    "        # Encode sentences\n",
    "        src_encoded = [self.encode_sentence(sent, self.src_vocab_to_idx) for sent in src_tokenized]\n",
    "        tgt_encoded = [self.encode_sentence(sent, self.tgt_vocab_to_idx) for sent in tgt_tokenized]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_padded = [self.pad_sequence(sent, self.max_src_len_actual) for sent in src_encoded]\n",
    "        tgt_padded = [self.pad_sequence(sent, self.max_tgt_len_actual) for sent in tgt_encoded]\n",
    "        \n",
    "        return src_padded, tgt_padded\n",
    "    \n",
    "    def get_vocab_info(self) -> Dict:\n",
    "        \"\"\"Get vocabulary information.\"\"\"\n",
    "        return {\n",
    "            'src_vocab_size': self.src_vocab_size,\n",
    "            'tgt_vocab_size': self.tgt_vocab_size,\n",
    "            'max_src_len': self.max_src_len_actual,\n",
    "            'max_tgt_len': self.max_tgt_len_actual,\n",
    "            'src_vocab_to_idx': self.src_vocab_to_idx,\n",
    "            'tgt_vocab_to_idx': self.tgt_vocab_to_idx,\n",
    "            'src_idx_to_vocab': self.src_idx_to_vocab,\n",
    "            'tgt_idx_to_vocab': self.tgt_idx_to_vocab\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for translation data.\"\"\"\n",
    "    \n",
    "    def __init__(self, src_data: List[List[int]], tgt_data: List[List[int]]):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx]), torch.tensor(self.tgt_data[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtUBZ07ccZ5R"
   },
   "source": [
    "## Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1751767481322,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FXO2Hf7IcZ5S"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -math.log(10000.0) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1751767482233,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "r4qNq9bAcZ5S"
   },
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead,\n",
    "                 src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead,\n",
    "                                          num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers,\n",
    "                                          dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask, mem_pad_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        out = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask,\n",
    "                               None, src_pad_mask, tgt_pad_mask, mem_pad_mask)\n",
    "        return self.generator(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1751767483074,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "TQMEmg9ScZ5S"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz, device):\n",
    "    return torch.triu(torch.full((sz, sz), float('-inf'), device=device), 1)\n",
    "\n",
    "def create_mask(src, tgt, pad_idx=0):\n",
    "    src_seq_len = src.size(0)\n",
    "    tgt_seq_len = tgt.size(0)\n",
    "    device = src.device  # assume both src and tgt are on the same device\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == pad_idx).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == pad_idx).transpose(0, 1)\n",
    "\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing translation text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src_sentence, src_vocab, tgt_vocab, ind_to_word_spa, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    sos_token = tgt_vocab[\"<SOS>\"]\n",
    "    eos_token = tgt_vocab[\"<EOS>\"]\n",
    "    pad_token = tgt_vocab.get(\"<PAD>\", 0)\n",
    "\n",
    "    # Encode source sentence\n",
    "    tokens = [\"<SOS>\"] + src_sentence.lower().strip().split() + [\"<EOS>\"]\n",
    "    src_indices = [src_vocab.get(tok, src_vocab.get(\"<UNK>\", 0)) for tok in tokens]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(1).to(device)  # [seq_len, 1]\n",
    "    src_mask = torch.zeros((src_tensor.size(0), src_tensor.size(0)), device=device).type(torch.bool)\n",
    "\n",
    "    # Encoder output\n",
    "    src_emb = model.positional_encoding(model.src_tok_emb(src_tensor))\n",
    "    memory = model.transformer.encoder(src_emb, src_mask)\n",
    "\n",
    "    # Start decoding with <SOS>\n",
    "    tgt_indices = [sos_token]\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_indices).unsqueeze(1).to(device)\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_tensor.size(0), device)\n",
    "\n",
    "        tgt_emb = model.positional_encoding(model.tgt_tok_emb(tgt_tensor))\n",
    "        out = model.transformer.decoder(tgt_emb, memory, tgt_mask)\n",
    "        out = model.generator(out)\n",
    "\n",
    "        next_token = out[-1].argmax(-1).item()\n",
    "\n",
    "        if next_token == eos_token:\n",
    "            break\n",
    "\n",
    "        tgt_indices.append(next_token)\n",
    "\n",
    "    decoded = [ind_to_word_spa.get(idx, \"\") for idx in tgt_indices[1:]]  # skip SOS\n",
    "    return \" \".join(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(eng_sentence, model, src_vocab, tgt_vocab, ind_to_word_spa, device):\n",
    "    return greedy_decode(model, eng_sentence, src_vocab, tgt_vocab, ind_to_word_spa\n",
    "                         ,device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging transalted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sample_translations(model, writer, epoch, src_vocab, tgt_vocab, tgt_idx_to_vocab, device, num_samples=5):\n",
    "    # Sample sentences to translate\n",
    "    sample_sentences = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"I love this place.\",\n",
    "        \"What time is it?\",\n",
    "        \"Thank you very much.\",\n",
    "        \"Good morning everyone.\",\n",
    "        \"I'm excited about dinner tonight.\"\n",
    "    ]\n",
    "    \n",
    "    model.eval()\n",
    "    translations_text = f\"## Epoch {epoch} Sample Translations\\n\\n\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, src_sentence in enumerate(sample_sentences[:num_samples]):\n",
    "            try:\n",
    "                # Translate using your existing translate function\n",
    "                translation = translate(src_sentence, model, src_vocab, tgt_vocab, tgt_idx_to_vocab, device)\n",
    "                \n",
    "                # Format for TensorBoard\n",
    "                translations_text += f\"**{src_sentence}** → **{translation}**\\n\\n\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                translations_text += f\"**{src_sentence}** → **Error: {str(e)}**\\n\\n\"\n",
    "    \n",
    "    # Log to TensorBoard\n",
    "    writer.add_text('Translations/Samples', translations_text, epoch)\n",
    "    writer.flush()\n",
    "    \n",
    "    print(f\"Logged {num_samples} sample translations for epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_33YYkdUcZ5T"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1751767490321,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "MHSvj9KwcZ5T"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, val_dataloader, preprocessor, optimizer, loss_fn, num_epochs, pad_idx=0, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    writer = torch.utils.tensorboard.SummaryWriter(f'runs/{MODEL_NAME}')\n",
    "    global_step = 1\n",
    "\n",
    "    vocab_to_ind_eng = preprocessor.src_vocab_to_idx\n",
    "    vocab_to_ind_spa = preprocessor.tgt_vocab_to_idx\n",
    "    ind_to_word_spa = preprocessor.tgt_idx_to_vocab\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        total_loss = 0\n",
    "        print(f\"Starting epoch {epoch}\")\n",
    "        dl_len = len(dataloader)\n",
    "        i = 1\n",
    "\n",
    "        model.train()\n",
    "        for eng_batch, spa_batch in dataloader:\n",
    "            print(f\"\\rStarting {i}/{dl_len}\", end=\"\")\n",
    "            i += 1\n",
    "            global_step += 1\n",
    "            eng_batch = eng_batch.transpose(0, 1).to(device) \n",
    "            spa_batch = spa_batch.transpose(0, 1).to(device) \n",
    "\n",
    "            tgt_input = spa_batch[:-1, :]\n",
    "            tgt_output = spa_batch[1:, :]\n",
    "\n",
    "            src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(eng_batch, tgt_input, pad_idx)\n",
    "\n",
    "            logits = model(\n",
    "                eng_batch,\n",
    "                tgt_input,\n",
    "                src_mask,\n",
    "                tgt_mask,\n",
    "                src_pad_mask,\n",
    "                tgt_pad_mask,\n",
    "                src_pad_mask  # memory padding mask\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Flatten predictions and targets\n",
    "            logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "            tgt_output_flat = tgt_output.reshape(-1)\n",
    "\n",
    "            loss = loss_fn(logits_flat, tgt_output_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "            writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        writer.add_scalar('Loss/train_epoch_avg', avg_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eng_batch, spa_batch in val_dataloader:\n",
    "                eng_batch = eng_batch.transpose(0, 1).to(device)\n",
    "                spa_batch = spa_batch.transpose(0, 1).to(device)\n",
    "\n",
    "                tgt_input = spa_batch[:-1, :]\n",
    "                tgt_output = spa_batch[1:, :]\n",
    "\n",
    "                src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = create_mask(eng_batch, tgt_input, pad_idx)\n",
    "\n",
    "                logits = model(\n",
    "                    eng_batch,\n",
    "                    tgt_input,\n",
    "                    src_mask,\n",
    "                    tgt_mask,\n",
    "                    src_pad_mask,\n",
    "                    tgt_pad_mask,\n",
    "                    src_pad_mask\n",
    "                )\n",
    "\n",
    "                logits_flat = logits.reshape(-1, logits.shape[-1])\n",
    "                tgt_output_flat = tgt_output.reshape(-1)\n",
    "                \n",
    "                val_loss = loss_fn(logits_flat, tgt_output_flat)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        writer.add_scalar('Loss/val_epoch_avg', avg_val_loss, epoch)\n",
    "\n",
    "        log_sample_translations(model, writer, epoch, vocab_to_ind_eng, vocab_to_ind_spa, ind_to_word_spa, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} — Train Loss: {avg_loss:.4f} — Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    writer.close()\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'./models/{MODEL_NAME}.pth')\n",
    "\n",
    "    return losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "text_path = get_text_path(FILE_NAME)\n",
    "\n",
    "with open(text_path, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")[:1000]\n",
    "\n",
    "preprocessor = TranslationPreprocessor(\n",
    "    src_tokenizer=SimpleWordTokenizer(),\n",
    "    tgt_tokenizer=SimpleWordTokenizer()\n",
    ")\n",
    "\n",
    "src_data, tgt_data = preprocessor.preprocess_data(lines)\n",
    "\n",
    "# Split into train and validation sets with fixed random seed\n",
    "np.random.seed(42)  # For reproducibility\n",
    "indices = np.random.permutation(len(src_data))\n",
    "train_size = int(0.9 * len(src_data))\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "train_src_data = [src_data[i] for i in train_indices]\n",
    "train_tgt_data = [tgt_data[i] for i in train_indices]\n",
    "val_src_data = [src_data[i] for i in val_indices]\n",
    "val_tgt_data = [tgt_data[i] for i in val_indices]\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TranslationDataset(train_src_data, train_tgt_data)\n",
    "val_dataset = TranslationDataset(val_src_data, val_tgt_data)\n",
    "train_dl = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "src_vocab_size = preprocessor.src_vocab_size\n",
    "tgt_vocab_size = preprocessor.tgt_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_github(local_path, repo_path, token, username=\"WillCable97\", repo=\"HomemadeTransformer\", branch=\"main\"):\n",
    "    \n",
    "    with open(local_path, \"rb\") as f:\n",
    "        content = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    url = f\"https://api.github.com/repos/{username}/{repo}/contents/{repo_path}\"\n",
    "    data = {\n",
    "        \"message\": f\"Upload {repo_path} from Colab\",\n",
    "        \"content\": content,\n",
    "        \"branch\": branch\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"token {token}\"}\n",
    "    r = requests.put(url, headers=headers, json=data)\n",
    "    if r.status_code == 201:\n",
    "        print(f\"✅ Uploaded {repo_path} to GitHub\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to upload {repo_path} — {r.status_code}: {r.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 530310,
     "status": "ok",
     "timestamp": 1751768025121,
     "user": {
      "displayName": "William Cable",
      "userId": "04663351666127960465"
     },
     "user_tz": -600
    },
    "id": "FCwVbyWBcZ5T",
    "outputId": "1bac3405-2569-4c00-b4e2-d314ad31e519"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Starting 1/29"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 29/29Logged 5 sample translations for epoch 0\n",
      "Epoch 1/10 — Train Loss: 5.1017 — Val Loss: 4.5857\n",
      "Starting epoch 1\n",
      "Starting 29/29Logged 5 sample translations for epoch 1\n",
      "Epoch 2/10 — Train Loss: 4.5807 — Val Loss: 4.5208\n",
      "Starting epoch 2\n",
      "Starting 29/29Logged 5 sample translations for epoch 2\n",
      "Epoch 3/10 — Train Loss: 4.3703 — Val Loss: 4.4191\n",
      "Starting epoch 3\n",
      "Starting 29/29Logged 5 sample translations for epoch 3\n",
      "Epoch 4/10 — Train Loss: 4.1559 — Val Loss: 4.1696\n",
      "Starting epoch 4\n",
      "Starting 2/29"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m loss_fn = nn.CrossEntropyLoss(ignore_index=\u001b[32m0\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_COLAB:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m#Save Log file\u001b[39;00m\n\u001b[32m     22\u001b[39m     log_dir= Path(\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, val_dataloader, preprocessor, optimizer, loss_fn, num_epochs, pad_idx, device)\u001b[39m\n\u001b[32m     50\u001b[39m tgt_output_flat = tgt_output.reshape(-\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m loss = loss_fn(logits_flat, tgt_output_flat)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m optimizer.step()\n\u001b[32m     56\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Model\n",
    "model = Seq2SeqTransformer(\n",
    "    num_encoder_layers=LAYERS,\n",
    "    num_decoder_layers=LAYERS,\n",
    "    emb_size=EMB_SIZE,\n",
    "    nhead=HEADS,\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Train it\n",
    "losses = train_model(model, train_dl, val_dl, preprocessor\n",
    "                     , optimizer, loss_fn\n",
    "                     , num_epochs=EPOCHS, pad_idx=0\n",
    "                     , device=DEVICE)\n",
    "\n",
    "if IS_COLAB:\n",
    "    #Save Log file\n",
    "    log_dir= Path(F\"./runs/{MODEL_NAME}\")\n",
    "    latest_event_file = max(log_dir.glob(\"events.out.tfevents.*\"), key=lambda f: f.stat().st_mtime, default=None)\n",
    "    upload_to_github(latest_event_file,\n",
    "                      f\"./notebooks/runs/{MODEL_NAME}/{latest_event_file.name}\"\n",
    "                      , GH_TOKEN)\n",
    "    \n",
    "    #Do the model\n",
    "    upload_to_github(f\"./models/{MODEL_NAME}.pth\",\n",
    "                      f\"./notebooks/models/{MODEL_NAME}.pth\"\n",
    "                      , GH_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
