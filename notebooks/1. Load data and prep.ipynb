{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homemadetransformer.config import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\Desktop\\AIPortfolio\\HomemadeTransformer\\data\n"
     ]
    }
   ],
   "source": [
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = DATA_DIR / \"raw\" / \"spa.txt\"\n",
    "\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    lines = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pair(line):\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "\n",
    "    if len(parts) < 2:\n",
    "        print(f\"PROBLEM: {line}\")\n",
    "        return None\n",
    "    eng = parts[0].strip()\n",
    "    spa = parts[1].strip().split(\"CC-BY\")[0].strip()  # Remove attribution text\n",
    "    return eng, spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pairs = [clean_pair(line) for line in lines]\n",
    "clean_pairs = [pair for pair in clean_pairs if pair is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', 'Ve.'),\n",
       " ('Go.', 'Vete.'),\n",
       " ('Go.', 'Vaya.'),\n",
       " ('Go.', 'Váyase.'),\n",
       " ('Hi.', 'Hola.'),\n",
       " ('Run!', '¡Corre!'),\n",
       " ('Run!', '¡Corran!'),\n",
       " ('Run!', '¡Huye!'),\n",
       " ('Run!', '¡Corra!'),\n",
       " ('Run!', '¡Corred!')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab objects\n",
    "split_eng_sent = [[\"<SOS>\"] + eng_sent.lower().strip().split() + [\"<EOS>\"] \n",
    "                  for eng_sent, spa_sent in clean_pairs]\n",
    "\n",
    "\n",
    "split_spa_sent = [[\"<SOS>\"] + spa_sent.lower().strip().split() + [\"<EOS>\"] \n",
    "                  for eng_sent, spa_sent in clean_pairs]\n",
    "\n",
    "eng_vocab = set(chain.from_iterable(split_eng_sent))\n",
    "spa_vocab = set(chain.from_iterable(split_spa_sent))\n",
    "\n",
    "vocab_to_ind_eng = {word: i for i, word in enumerate(eng_vocab)}\n",
    "vocab_to_ind_spa = {word: i for i, word in enumerate(spa_vocab)}\n",
    "\n",
    "ind_to_word_eng = {i:w for w, i in vocab_to_ind_eng.items()}\n",
    "ind_to_word_spa = {i:w for w, i in vocab_to_ind_spa.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode the sentences\n",
    "def encode_sentence_vectors(sentence: list, vocab_to_ind: dict):\n",
    "    return [vocab_to_ind[word] for word in sentence]\n",
    "\n",
    "\n",
    "eng_senteces_encoded = [encode_sentence_vectors(sentence, vocab_to_ind_eng)\n",
    "                        for sentence in split_eng_sent]\n",
    "\n",
    "spa_senteces_encoded = [encode_sentence_vectors(sentence, vocab_to_ind_spa)\n",
    "                        for sentence in split_spa_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'go.', '<EOS>']\n",
      "[17490, 24991, 22338]\n"
     ]
    }
   ],
   "source": [
    "print(split_eng_sent[0])\n",
    "print(eng_senteces_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
